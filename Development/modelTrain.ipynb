{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torchvision.models.vgg import VGG19_Weights\n",
    "import torchvision.transforms.functional as F\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SRResNet_Up2 import SRResNet\n",
    "from EDSR_Extended import EDSR\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.vgg import VGG19_Weights\n",
    "#from SRResNetY import SRResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGContentLoss(nn.Module):\n",
    "    def __init__(self, layer_ids=[4, 9, 18, 27], use_pretrained=True):\n",
    "        super(VGGContentLoss, self).__init__()\n",
    "        # Carregar a VGG19 pré-treinada no ImageNet\n",
    "        vgg = models.vgg19(pretrained=use_pretrained).features\n",
    "        # Mantemos as camadas até a layer que queremos (maxpool1, maxpool2, etc.)\n",
    "        self.vgg_layers = nn.Sequential(*[vgg[i] for i in range(max(layer_ids)+1)])\n",
    "        # Definir quais camadas serão utilizadas para o cálculo do loss\n",
    "        self.layer_ids = layer_ids\n",
    "        # Congelemos os parâmetros da VGG (não será treinada)\n",
    "        for param in self.vgg_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, img_real, img_fake):\n",
    "        # Extrair as características das imagens reais e geradas\n",
    "        features_real = self.extract_features(img_real)\n",
    "        features_fake = self.extract_features(img_fake)\n",
    "\n",
    "        # Content Loss: diferença L2 entre as características\n",
    "        content_loss = 0.0\n",
    "        for real, fake in zip(features_real, features_fake):\n",
    "            content_loss += torch.nn.functional.mse_loss(fake, real)\n",
    "\n",
    "        return content_loss\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        features = []\n",
    "        for i, layer in enumerate(self.vgg_layers):\n",
    "            x = layer(x)\n",
    "            if i in self.layer_ids:\n",
    "                features.append(x)\n",
    "            \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 22\n",
    "num_workers = 1\n",
    "num_epochs = 1\n",
    "save_interval = 20\n",
    "betas = (0.5, 0.9)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"dataset/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.CenterCrop((256, 256)),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5, 0.5, 0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "\n",
    "transform_LR = transforms.Compose([\n",
    "  transforms.CenterCrop((256, 256)),\n",
    "  transforms.Resize((128,128)),\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize((0.5, 0.5, 0.5), (0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "hr_dataset = ImageFolder(root=data_path + \"lr\", transform= transform)\n",
    "lr_dataset = ImageFolder(root=data_path + \"lr\", transform=transform_LR)\n",
    "\n",
    "hr_data_loader = DataLoader(hr_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "lr_data_loader = DataLoader(lr_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "#vgg_loss = VGGContentLoss(layer_ids=[4, 9, 18, 27]).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFolders(name):\n",
    "  os.mkdir(name)\n",
    "  os.mkdir(name + '/training_images')\n",
    "  os.mkdir(name + '/snapshots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EDSR()\n",
    "#model.load_state_dict(torch.load('SRResNet-Backup - Actuual/model.pt'))\n",
    "models = [model] #[ESRT(),RNAN(), SRResnet(), SRResnetExtended(), RNANExtended()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trainModel(model, loss_function, LRS, num_epochs):\n",
    "  error_track = []\n",
    "  model.to(device)\n",
    "  makeFolders(model.name)\n",
    "  for epoch, lr in zip(range(0, num_epochs), LRS):\n",
    "    optimizer_gen = optim.Adam(model.parameters(), lr = lr, betas = betas)\n",
    "    for i, (hr_images, lr_images) in enumerate(zip(hr_data_loader, lr_data_loader)):\n",
    "      hr_images = hr_images[0].to(device).float()\n",
    "      lr_images = lr_images[0].to(device).float()\n",
    "      \n",
    "      sr_images = model(lr_images)\n",
    "      g_loss_content = loss_function(sr_images, hr_images)\n",
    "      #g_loss_content = vgg_loss(sr_images, hr_images)  #+ (loss_function(sr_images, hr_images) * 0.1)\n",
    "      g_loss_content.backward()\n",
    "      \n",
    "      optimizer_gen.step()\n",
    "      \n",
    "      model.zero_grad()\n",
    "      optimizer_gen.zero_grad()\n",
    "      \n",
    "      if (i + 1) % save_interval == 1:\n",
    "        print(g_loss_content.item())\n",
    "        error_track.append(g_loss_content.item())\n",
    "      if (i) % 50 == 0:\n",
    "        torch.save(model.state_dict(), f\"{model.name}/snapshots/epoch_{epoch}_batch_{i}.pt\")\n",
    "        save_image(sr_images, f\"{model.name}/training_images/SR_epoch_{epoch}_batch_{i}.png\", normalize = True)\n",
    "        save_image(hr_images, f\"{model.name}/training_images/HR_epoch_{epoch}_batch_{i}.png\", normalize = True)\n",
    "        save_image(lr_images, f\"{model.name}/training_images/LR_epoch_{epoch}_batch_{i}.png\", normalize = True)\n",
    "  return error_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLRList(initial, destiny, epochs):\n",
    "  dif = destiny - initial\n",
    "  dif /= epochs\n",
    "  return [initial + dif * i for i in range(0, epochs)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavosmc/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/conda/conda-bld/pytorch_1712608853085/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3264651596546173\n",
      "0.036086611449718475\n",
      "0.028496479615569115\n",
      "0.03048541024327278\n",
      "0.02195902355015278\n",
      "0.02428966574370861\n",
      "0.022302910685539246\n",
      "0.02014380507171154\n",
      "0.02013089507818222\n",
      "0.018002958968281746\n",
      "0.016507795080542564\n",
      "0.015390399843454361\n",
      "0.024000216275453568\n",
      "0.02085338719189167\n",
      "0.01864708960056305\n",
      "0.01859557442367077\n",
      "0.010455998592078686\n",
      "0.01255401223897934\n",
      "0.0136759327724576\n",
      "0.015421737916767597\n",
      "0.013191530480980873\n",
      "0.013690485619008541\n",
      "0.022672588005661964\n",
      "0.01504336018115282\n",
      "0.012153388001024723\n",
      "0.015066048130393028\n",
      "0.018631551414728165\n",
      "0.012017378583550453\n",
      "0.012952769175171852\n",
      "0.010023247450590134\n",
      "0.01201297715306282\n",
      "0.012895997613668442\n",
      "0.014828398823738098\n",
      "0.008437300100922585\n",
      "0.015218903310596943\n",
      "0.013938026502728462\n",
      "0.0137669388204813\n",
      "0.01197677943855524\n",
      "0.01400416623800993\n",
      "0.009313098154962063\n",
      "0.012113920412957668\n",
      "0.012830597348511219\n",
      "0.01725752465426922\n",
      "0.010448230430483818\n",
      "0.00981107261031866\n",
      "0.014631197787821293\n",
      "0.01048300415277481\n",
      "0.017140096053481102\n",
      "0.011465269140899181\n",
      "0.013045291416347027\n",
      "0.011972938664257526\n",
      "0.012915752828121185\n",
      "0.010533677414059639\n",
      "0.008656062185764313\n",
      "0.011559801176190376\n",
      "0.013418791815638542\n",
      "0.011922827921807766\n",
      "0.01068167109042406\n",
      "0.011945988982915878\n",
      "0.012425866909325123\n",
      "0.019598275423049927\n",
      "0.0108882375061512\n",
      "0.011148511432111263\n",
      "0.013658767566084862\n",
      "0.01035888958722353\n",
      "0.012046870775520802\n",
      "0.011852537281811237\n",
      "0.012997214682400227\n",
      "0.010427765548229218\n",
      "0.011085893958806992\n",
      "0.009403477422893047\n",
      "0.011237899772822857\n",
      "0.010482859797775745\n",
      "0.011935924179852009\n",
      "0.012384738773107529\n",
      "0.013267486356198788\n",
      "0.010755905881524086\n",
      "0.0122213838621974\n",
      "0.01760602742433548\n",
      "0.016331730410456657\n",
      "0.01231066882610321\n",
      "0.00941340159624815\n",
      "0.016123244538903236\n",
      "0.010097000747919083\n",
      "0.011919713579118252\n",
      "0.01255760807543993\n",
      "0.010249201208353043\n",
      "0.010512632317841053\n",
      "0.011721467599272728\n",
      "0.009505032561719418\n",
      "0.011510643176734447\n",
      "0.010088720358908176\n",
      "0.010178458876907825\n",
      "0.008377977646887302\n",
      "0.010403148829936981\n",
      "0.009223944507539272\n",
      "0.009557324461638927\n",
      "0.010770810768008232\n",
      "0.010614100843667984\n",
      "0.01594587415456772\n",
      "0.01571209914982319\n",
      "0.009782010689377785\n",
      "0.008903378620743752\n",
      "0.00943879596889019\n",
      "0.008969543501734734\n",
      "0.008484986610710621\n",
      "0.014742822386324406\n",
      "0.011996272951364517\n",
      "0.010051759891211987\n",
      "0.013429797254502773\n",
      "0.007676305249333382\n",
      "0.00950655061751604\n",
      "0.010593206621706486\n",
      "0.009526877664029598\n",
      "0.008114761672914028\n",
      "0.012353377416729927\n",
      "0.009530347771942616\n",
      "0.010755140334367752\n",
      "0.009790166281163692\n",
      "0.010209199041128159\n",
      "0.011311139911413193\n",
      "0.006619573570787907\n",
      "0.009778357110917568\n",
      "0.00979086197912693\n",
      "0.006767797749489546\n",
      "0.0074909948743879795\n",
      "0.009091801941394806\n",
      "0.012612663209438324\n",
      "0.017860418185591698\n",
      "0.007857976481318474\n",
      "0.015428335405886173\n",
      "0.006883388385176659\n",
      "0.009143094532191753\n",
      "0.006902061868458986\n",
      "0.008663839660584927\n",
      "0.01042261254042387\n",
      "0.00915184523910284\n",
      "0.00818421971052885\n",
      "0.011678673326969147\n",
      "0.007796313147991896\n",
      "0.007429677993059158\n",
      "0.009813583455979824\n",
      "0.012149482034146786\n",
      "0.010438242927193642\n",
      "0.013338183984160423\n",
      "0.009868216700851917\n",
      "0.008395674638450146\n",
      "0.006521397270262241\n",
      "0.014206509105861187\n",
      "0.010360952466726303\n",
      "0.011464126408100128\n",
      "0.007906507700681686\n",
      "0.014137627556920052\n",
      "0.00717984139919281\n",
      "0.008842802606523037\n",
      "0.00633121095597744\n",
      "0.009906447492539883\n",
      "0.01549447514116764\n",
      "0.0065767508931458\n",
      "0.0074838800355792046\n",
      "0.010287974961102009\n",
      "0.009799052961170673\n",
      "0.011606646701693535\n",
      "0.010677440091967583\n",
      "0.008185301907360554\n",
      "0.011378657072782516\n",
      "0.007820304483175278\n",
      "0.012289678677916527\n",
      "0.009321413934230804\n",
      "0.008481382392346859\n",
      "0.010264313779771328\n",
      "0.01173327211290598\n",
      "0.008638210594654083\n",
      "0.0066587780602276325\n",
      "0.009577258490025997\n",
      "0.007766266819089651\n",
      "0.01270068809390068\n",
      "0.010676414705812931\n",
      "0.008776373229920864\n",
      "0.009020881727337837\n",
      "0.008042771369218826\n",
      "0.010022367350757122\n",
      "0.005991697311401367\n",
      "0.009279442951083183\n",
      "0.009854976087808609\n",
      "0.011077363044023514\n",
      "0.010280237533152103\n",
      "0.008662588894367218\n",
      "0.010788796469569206\n",
      "0.013755002059042454\n",
      "0.008473129011690617\n",
      "0.010330216027796268\n",
      "0.006208799313753843\n",
      "0.008381123654544353\n",
      "0.011422153562307358\n",
      "0.007737239357084036\n",
      "0.0071892025880515575\n",
      "0.009113872423768044\n",
      "0.010034437291324139\n",
      "0.01585713028907776\n",
      "0.008596922270953655\n",
      "0.010494961403310299\n",
      "0.011763859540224075\n",
      "0.006072784308344126\n",
      "0.01050224993377924\n",
      "0.010205987840890884\n",
      "0.005767735652625561\n",
      "0.00979318656027317\n",
      "0.014464419335126877\n",
      "0.009851627051830292\n",
      "0.010582850314676762\n",
      "0.005740433000028133\n",
      "0.01043016742914915\n",
      "0.010979621671140194\n",
      "0.007143157068639994\n",
      "0.007520496845245361\n",
      "0.010800349526107311\n",
      "0.009035549126565456\n",
      "0.008723360486328602\n",
      "0.007890609093010426\n",
      "0.007274377625435591\n",
      "0.009485598653554916\n",
      "0.008450847119092941\n",
      "0.007859260775148869\n",
      "0.0062216222286224365\n",
      "0.009183526039123535\n",
      "0.013024533167481422\n",
      "0.008167154155671597\n",
      "0.010654249228537083\n",
      "0.009244180284440517\n",
      "0.008483660407364368\n",
      "0.011762497946619987\n",
      "0.006488105747848749\n",
      "0.012009046971797943\n",
      "0.011092078872025013\n",
      "0.007332867942750454\n",
      "0.0072954739443957806\n",
      "0.009012559428811073\n",
      "0.006760087329894304\n",
      "0.008007805794477463\n",
      "0.007225280627608299\n",
      "0.010349326767027378\n",
      "0.011323829181492329\n",
      "0.008242527954280376\n",
      "0.007896649651229382\n",
      "0.01713327318429947\n",
      "0.008653024211525917\n",
      "0.0062307133339345455\n",
      "0.0070256502367556095\n",
      "0.011431124992668629\n",
      "0.009141767397522926\n",
      "0.01281411200761795\n",
      "0.009566592052578926\n",
      "0.007604900747537613\n",
      "0.00878368504345417\n",
      "0.009456965140998363\n",
      "0.00897168554365635\n",
      "0.007333660032600164\n",
      "0.010187814012169838\n",
      "0.008402766659855843\n",
      "0.008045179769396782\n",
      "0.007746650837361813\n",
      "0.007909745909273624\n",
      "0.009703284129500389\n",
      "0.009676999412477016\n",
      "0.008781444281339645\n",
      "0.014375590719282627\n",
      "0.006313294172286987\n",
      "0.006569429766386747\n",
      "0.012293717823922634\n",
      "0.0054860832169651985\n",
      "0.00706133246421814\n",
      "0.010192327201366425\n",
      "0.007746046409010887\n",
      "0.010562894865870476\n",
      "0.008095075376331806\n",
      "0.009458179585635662\n",
      "0.00902580562978983\n",
      "0.007555716205388308\n",
      "0.008379707112908363\n",
      "0.007533655967563391\n",
      "0.010115796700119972\n",
      "0.009649292565882206\n",
      "0.010791164822876453\n",
      "0.011611346155405045\n",
      "0.005699620582163334\n",
      "0.0066428883001208305\n",
      "0.008107185363769531\n",
      "0.0102975033223629\n",
      "0.0077689229510724545\n",
      "0.008930204436182976\n",
      "0.015255177393555641\n",
      "0.009715919382870197\n",
      "0.007614755537360907\n",
      "0.010466260835528374\n",
      "0.01215348206460476\n",
      "0.0074364724569022655\n",
      "0.009027717635035515\n",
      "0.005922847893089056\n",
      "0.008380698040127754\n",
      "0.00889313593506813\n",
      "0.011281893588602543\n",
      "0.005622440483421087\n",
      "0.010629943571984768\n",
      "0.009812971577048302\n",
      "0.007831637747585773\n",
      "0.008794713765382767\n",
      "0.010388961993157864\n",
      "0.006622674874961376\n",
      "0.008396869525313377\n",
      "0.009608395397663116\n",
      "0.011487576179206371\n",
      "0.0066593424417078495\n",
      "0.006965681444853544\n",
      "0.011873292736709118\n",
      "0.007358364295214415\n",
      "0.01205895934253931\n",
      "0.008488385938107967\n",
      "0.00919447373598814\n",
      "0.007996723987162113\n",
      "0.009865004569292068\n",
      "0.007993098348379135\n",
      "0.006370154209434986\n",
      "0.008503521792590618\n",
      "0.010258583351969719\n",
      "0.008963353931903839\n",
      "0.008113397285342216\n",
      "0.009518240578472614\n",
      "0.009203512221574783\n",
      "0.015456445515155792\n",
      "0.008438879624009132\n",
      "0.007881966419517994\n",
      "0.010696228593587875\n",
      "0.0077636693604290485\n",
      "0.009848669171333313\n",
      "0.008841280825436115\n",
      "0.010642011649906635\n",
      "0.008714199997484684\n",
      "0.008947745896875858\n",
      "0.0072515616193413734\n",
      "0.008939012885093689\n",
      "0.007733030244708061\n",
      "0.009196738712489605\n",
      "0.009854091331362724\n",
      "0.010547847487032413\n",
      "0.008103329688310623\n",
      "0.009648949839174747\n",
      "0.01553331222385168\n",
      "0.013655006885528564\n",
      "0.008959099650382996\n",
      "0.007030550390481949\n",
      "0.013134721666574478\n",
      "0.008185961283743382\n",
      "0.009659974835813046\n",
      "0.010611394420266151\n",
      "0.008338399231433868\n",
      "0.008426208049058914\n",
      "0.009463613852858543\n",
      "0.00771947531029582\n",
      "0.00926126167178154\n",
      "0.007589227519929409\n",
      "0.008368772454559803\n",
      "0.006903886329382658\n",
      "0.008215525187551975\n",
      "0.007306638173758984\n",
      "0.007916695438325405\n",
      "0.008569000288844109\n",
      "0.00865474808961153\n",
      "0.013814013451337814\n",
      "0.013217274099588394\n",
      "0.008261051960289478\n",
      "0.007336976006627083\n",
      "0.007979911752045155\n",
      "0.007668097037822008\n",
      "0.007126000709831715\n",
      "0.012417552061378956\n",
      "0.009823627769947052\n",
      "0.008586559444665909\n",
      "0.011623234488070011\n",
      "0.006370468530803919\n",
      "0.008090192452073097\n",
      "0.008629134856164455\n",
      "0.007362643722444773\n",
      "0.006878348998725414\n",
      "0.010643159970641136\n",
      "0.007840385660529137\n",
      "0.00925141666084528\n",
      "0.00857485830783844\n",
      "0.00845238659530878\n",
      "0.00985395722091198\n",
      "0.005466610658913851\n",
      "0.00834997370839119\n",
      "0.008318526670336723\n",
      "0.005732979159802198\n",
      "0.0065213218331336975\n",
      "0.007520764134824276\n",
      "0.011098042130470276\n",
      "0.01606435328722\n",
      "0.006838216912001371\n",
      "0.012990202754735947\n",
      "0.005748681258410215\n",
      "0.00783640705049038\n",
      "0.0062139020301401615\n",
      "0.007552177645266056\n",
      "0.009045828133821487\n",
      "0.007894598878920078\n",
      "0.007272342219948769\n",
      "0.010232714004814625\n",
      "0.006600501947104931\n",
      "0.006309350486844778\n",
      "0.008604004047811031\n",
      "0.011082833632826805\n",
      "0.009420132264494896\n",
      "0.01216046791523695\n",
      "0.008746258914470673\n",
      "0.007270365953445435\n",
      "0.0055296639911830425\n",
      "0.01274997927248478\n",
      "0.0086611807346344\n",
      "0.009525652974843979\n",
      "0.0071253650821745396\n",
      "0.012678643688559532\n",
      "0.006313957739621401\n",
      "0.007087395526468754\n",
      "0.005813822615891695\n",
      "0.008679189719259739\n",
      "0.013709969818592072\n",
      "0.005818469449877739\n",
      "0.006640581879764795\n",
      "0.009434090927243233\n",
      "0.008922628127038479\n",
      "0.010323235765099525\n",
      "0.009397604502737522\n",
      "0.007431781850755215\n",
      "0.010037281550467014\n",
      "0.007073519751429558\n",
      "0.01117415726184845\n",
      "0.008297142572700977\n",
      "0.00762763898819685\n",
      "0.00943099707365036\n",
      "0.01033850573003292\n",
      "0.007474311627447605\n",
      "0.006036222446709871\n",
      "0.00839495100080967\n",
      "0.006985736079514027\n",
      "0.011636432260274887\n",
      "0.009773788042366505\n",
      "0.008041583932936192\n",
      "0.007628352381289005\n",
      "0.0071856556460261345\n",
      "0.009200121276080608\n",
      "0.005428234115242958\n",
      "0.008512425236403942\n",
      "0.009120423346757889\n",
      "0.00988386757671833\n",
      "0.009418829344213009\n",
      "0.00802663154900074\n",
      "0.009773352183401585\n",
      "0.01267955545336008\n",
      "0.007727585267275572\n",
      "0.009768216870725155\n",
      "0.005756445694714785\n",
      "0.007577146869152784\n",
      "0.010549494996666908\n",
      "0.007055523339658976\n",
      "0.006550146732479334\n",
      "0.008562218397855759\n",
      "0.009326719678938389\n",
      "0.014869041740894318\n",
      "0.00797157920897007\n",
      "0.00964012648910284\n",
      "0.0106357391923666\n",
      "0.005326544865965843\n",
      "0.009754585102200508\n",
      "0.00896737165749073\n",
      "0.005378118250519037\n",
      "0.009015513584017754\n",
      "0.013027223758399487\n",
      "0.009036303497850895\n",
      "0.00965256430208683\n",
      "0.005406256299465895\n",
      "0.009633996523916721\n",
      "0.010131480172276497\n",
      "0.006416358519345522\n",
      "0.006949002388864756\n",
      "0.010121308267116547\n",
      "0.008457714691758156\n",
      "0.008297855034470558\n",
      "0.007363275159150362\n",
      "0.006632721051573753\n",
      "0.008871156722307205\n",
      "0.008016682229936123\n",
      "0.007319789379835129\n",
      "0.005681551061570644\n",
      "0.008580004796385765\n",
      "0.012168986722826958\n",
      "0.007250347174704075\n",
      "0.009878012351691723\n",
      "0.008517282083630562\n",
      "0.007731566205620766\n",
      "0.010890910401940346\n",
      "0.006006033159792423\n",
      "0.011387436650693417\n",
      "0.010247485712170601\n",
      "0.0066733332350850105\n",
      "0.006581070367246866\n",
      "0.008472578600049019\n",
      "0.006186239887028933\n",
      "0.007329478394240141\n",
      "0.006740926764905453\n",
      "0.009784982539713383\n",
      "0.010711254552006721\n",
      "0.007658953312784433\n",
      "0.007452819030731916\n",
      "0.016034958884119987\n",
      "0.008109801448881626\n",
      "0.005890619941055775\n",
      "0.006443419493734837\n",
      "0.010865534655749798\n",
      "0.0086514288559556\n",
      "0.011947684921324253\n",
      "0.008956634439527988\n",
      "0.007106036879122257\n",
      "0.008339540101587772\n",
      "0.009030886925756931\n",
      "0.008485843427479267\n",
      "0.006948533933609724\n",
      "0.009599377401173115\n",
      "0.00791544746607542\n",
      "0.007526031229645014\n",
      "0.007237435784190893\n",
      "0.007187745533883572\n",
      "0.009284142404794693\n",
      "0.009192781522870064\n",
      "0.008326067589223385\n",
      "0.01366069819778204\n",
      "0.005744350608438253\n",
      "0.006137361284345388\n",
      "0.011726513504981995\n",
      "0.005123221781104803\n",
      "0.006696182303130627\n",
      "0.009713350795209408\n",
      "0.007385231554508209\n",
      "0.010044249705970287\n",
      "0.007508468814194202\n",
      "0.008999842219054699\n",
      "0.008452877402305603\n",
      "0.007051924243569374\n",
      "0.007887149229645729\n",
      "0.007133410312235355\n",
      "0.009440731257200241\n",
      "0.00888840388506651\n",
      "0.010255320928990841\n",
      "0.011109107173979282\n",
      "0.005378805100917816\n",
      "0.006256705150008202\n",
      "0.007719079032540321\n",
      "0.009893758222460747\n",
      "0.007239685859531164\n",
      "0.008432318456470966\n",
      "0.014456762000918388\n",
      "0.009158378466963768\n",
      "0.007053340319544077\n",
      "0.009970949962735176\n",
      "0.011492538265883923\n",
      "0.00694953091442585\n",
      "0.008497563190758228\n",
      "0.0056093716993927956\n",
      "0.007935754954814911\n",
      "0.008432662114501\n",
      "0.01055443100631237\n",
      "0.005353308282792568\n",
      "0.01005442813038826\n",
      "0.009336363524198532\n",
      "0.0073544783517718315\n",
      "0.008278148248791695\n",
      "0.00970594584941864\n",
      "0.0062651135958731174\n",
      "0.007892466150224209\n",
      "0.009129161015152931\n",
      "0.010614554397761822\n",
      "0.006230555474758148\n",
      "0.006605456583201885\n",
      "0.011499677784740925\n",
      "0.006959171034395695\n",
      "0.011267895810306072\n",
      "0.008139805868268013\n",
      "0.008741884492337704\n",
      "0.007524312473833561\n",
      "0.009386302903294563\n",
      "0.007536589168012142\n",
      "0.005934210494160652\n",
      "0.008020526729524136\n",
      "0.009800651110708714\n",
      "0.008577002212405205\n",
      "0.007715781684964895\n",
      "0.009254389442503452\n",
      "0.008679120801389217\n",
      "0.014881912618875504\n",
      "0.008141382597386837\n",
      "0.007438211236149073\n",
      "0.010189947672188282\n",
      "0.007319206837564707\n",
      "0.009422353468835354\n",
      "0.008435417897999287\n",
      "0.01030133105814457\n",
      "0.008110885508358479\n",
      "0.008480452932417393\n",
      "0.006868470460176468\n",
      "0.00852377898991108\n",
      "0.007261614315211773\n",
      "0.00866011530160904\n",
      "0.009389299899339676\n",
      "0.010136826895177364\n",
      "0.0075447577983140945\n",
      "0.009156019426882267\n",
      "0.015286789275705814\n",
      "0.013160649687051773\n",
      "0.008492403663694859\n",
      "0.006682251114398241\n",
      "0.012531299144029617\n",
      "0.007822366431355476\n",
      "0.009262820705771446\n",
      "0.010164675302803516\n",
      "0.007903702557086945\n",
      "0.008040468208491802\n",
      "0.008948300033807755\n",
      "0.0071676671504974365\n",
      "0.008711717091500759\n",
      "0.007172239013016224\n",
      "0.007995767518877983\n",
      "0.0065331049263477325\n",
      "0.007761386688798666\n",
      "0.006946634966880083\n",
      "0.007364030461758375\n",
      "0.008131537586450577\n",
      "0.00820767693221569\n",
      "0.013385513797402382\n",
      "0.012683678418397903\n",
      "0.007917279377579689\n",
      "0.006988035514950752\n",
      "0.007640439551323652\n",
      "0.007296484895050526\n",
      "0.0066830068826675415\n",
      "0.011943920515477657\n",
      "0.009381666779518127\n",
      "0.008145476691424847\n",
      "0.011040604673326015\n",
      "0.006063530687242746\n",
      "0.007693004794418812\n",
      "0.00824075285345316\n",
      "0.007044692523777485\n",
      "0.006603063549846411\n",
      "0.010268623940646648\n",
      "0.0075207180343568325\n",
      "0.008993934839963913\n",
      "0.008306041359901428\n",
      "0.008104795590043068\n",
      "0.00946728140115738\n",
      "0.005218681413680315\n",
      "0.007970555685460567\n",
      "0.008010892197489738\n",
      "0.005447139963507652\n",
      "0.00613659992814064\n",
      "0.007138070184737444\n",
      "0.010698848403990269\n",
      "0.01557786762714386\n",
      "0.006433155853301287\n",
      "0.012481830082833767\n",
      "0.005510531831532717\n",
      "0.007481419946998358\n",
      "0.005907792132347822\n",
      "0.007290700450539589\n",
      "0.008706768974661827\n",
      "0.007588125765323639\n",
      "0.006995965726673603\n",
      "0.009762381203472614\n",
      "0.00637010158970952\n",
      "0.005988358054310083\n",
      "0.00824147928506136\n",
      "0.010802085511386395\n",
      "0.009149770252406597\n",
      "0.011834373697638512\n",
      "0.00842997431755066\n",
      "0.006917706225067377\n",
      "0.005246198736131191\n",
      "0.012423855252563953\n",
      "0.008323424495756626\n",
      "0.009126572869718075\n",
      "0.006891891825944185\n",
      "0.012159165926277637\n",
      "0.006032729521393776\n",
      "0.006727003026753664\n",
      "0.0054557956755161285\n",
      "0.008438270539045334\n",
      "0.013388756662607193\n",
      "0.005639491602778435\n",
      "0.006503666285425425\n",
      "0.009185373783111572\n",
      "0.008641562424600124\n",
      "0.009904204867780209\n",
      "0.00895764958113432\n",
      "0.007191195618361235\n",
      "0.00948259700089693\n",
      "0.00678016385063529\n",
      "0.010811184532940388\n",
      "0.00797145627439022\n",
      "0.007286006584763527\n",
      "0.009169946424663067\n",
      "0.00990412849932909\n",
      "0.0071295201778411865\n",
      "0.005761845037341118\n",
      "0.00807629618793726\n",
      "0.006713523995131254\n",
      "0.011326364241540432\n",
      "0.009505197405815125\n",
      "0.007791418116539717\n",
      "0.007451022509485483\n",
      "0.006884593982249498\n",
      "0.008888770826160908\n",
      "0.00517542427405715\n",
      "0.008187131956219673\n",
      "0.008880759589374065\n",
      "0.009364058263599873\n",
      "0.009109867736697197\n",
      "0.007790094241499901\n",
      "0.009487439878284931\n",
      "0.01234563160687685\n",
      "0.007382259704172611\n",
      "0.009572450071573257\n",
      "0.005505618639290333\n",
      "0.007300470490008593\n",
      "0.010146058164536953\n",
      "0.006757332943379879\n",
      "0.0063136303797364235\n",
      "0.008340881206095219\n",
      "0.008997749537229538\n",
      "0.01452733762562275\n",
      "0.007690041791647673\n",
      "0.009312250651419163\n",
      "0.010218219831585884\n",
      "0.005141379777342081\n",
      "0.009446083568036556\n",
      "0.00853712297976017\n",
      "0.005179616156965494\n",
      "0.008802509866654873\n",
      "0.012705318629741669\n",
      "0.008638881146907806\n",
      "0.009350760839879513\n",
      "0.005245166830718517\n",
      "0.009429072961211205\n",
      "0.00967252068221569\n",
      "0.0061889151111245155\n",
      "0.0067376685328781605\n",
      "0.00979479867964983\n",
      "0.008186048828065395\n",
      "0.008159679360687733\n",
      "0.007115236483514309\n",
      "0.0064461370930075645\n",
      "0.008632417768239975\n",
      "0.007806306704878807\n",
      "0.00706621864810586\n",
      "0.005468397401273251\n",
      "0.008383594453334808\n",
      "0.011943123303353786\n",
      "0.006891052238643169\n",
      "0.009550932794809341\n",
      "0.008310270495712757\n",
      "0.007438804022967815\n",
      "0.01060902513563633\n",
      "0.005818425212055445\n",
      "0.01092145498842001\n",
      "0.009906078688800335\n",
      "0.00646438542753458\n",
      "0.0063515156507492065\n",
      "0.008264901116490364\n",
      "0.00595654034987092\n",
      "0.007079274859279394\n",
      "0.006536607630550861\n",
      "0.009526225738227367\n",
      "0.010457930155098438\n",
      "0.007364631164819002\n",
      "0.007248114328831434\n",
      "0.015528778545558453\n",
      "0.007863459177315235\n",
      "0.005690452177077532\n",
      "0.0061698658391833305\n",
      "0.01058882661163807\n",
      "0.008219366893172264\n",
      "0.011622479185461998\n",
      "0.008695137687027454\n",
      "0.006968297529965639\n",
      "0.008098989725112915\n",
      "0.008762955665588379\n",
      "0.00825099740177393\n",
      "0.006771650165319443\n",
      "0.009407740086317062\n",
      "0.007740057073533535\n",
      "0.007315368857234716\n",
      "0.007025734055787325\n",
      "0.006966585759073496\n",
      "0.009048460982739925\n",
      "0.008945555426180363\n",
      "0.008016109466552734\n",
      "0.013317159377038479\n",
      "0.005561881233006716\n",
      "0.005952276289463043\n",
      "0.0114916255697608\n",
      "0.0049749622121453285\n",
      "0.006531157530844212\n",
      "0.009447941556572914\n",
      "0.007202527951449156\n",
      "0.009837312623858452\n"
     ]
    }
   ],
   "source": [
    "error_tracks = {}\n",
    "for model in models:\n",
    "  error_tracks[model.name] = trainModel(model, nn.MSELoss(), makeLRList(0.0001, 0.000001, 5), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(error_tracks)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(error_tracks)\n",
    "df['Batch'] = np.asarray(df.index)\n",
    "df.to_csv('Relatorio de Treino EDSR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(palette='deep')\n",
    "ax = sns.lineplot(df, x='Batch', y='EDSR')\n",
    "ax.set_title('SRResNet sem BN')\n",
    "ax.set_ylabel('')\n",
    "plt.savefig('EDSR/ErrorCurve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(palette='deep')\n",
    "ax = sns.lineplot(df, x='Batch', y='EDSR')\n",
    "ax.set_title('SRResNet-Extended')\n",
    "ax.set_ylabel('')\n",
    "plt.savefig('SRResNet-Extended/ErrorCurve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.DataFrame({'Iteração do Batch': range(0, len(error_track)), 'MSE' : error_track_array})\n",
    "sns.set_theme()\n",
    "ax = sns.lineplot(df, x='Iteração do Batch', y='MSE')\n",
    "ax.set_title(f'{model.name}')\n",
    "plt.savefig(f'{model.name}/ErrorCurve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for model in models:\n",
    "   Modelo na GPU\n",
    "  model.to(device)\n",
    "  content_loss = nn.MSELoss()\n",
    "   Otimizador\n",
    "  optimizer_gen = optim.Adam(model.parameters(), lr = lr, betas = betas)\n",
    "  error_track = [ ]\n",
    "  error_lines = []\n",
    "  makeFolders(model.name)\n",
    "  Treino\n",
    "  for epoch in range(0, num_epochs):\n",
    "    for i, (hr_images, lr_images) in enumerate(zip(hr_data_loader, lr_data_loader)):\n",
    "      hr_images = hr_images[0].to(device).float()\n",
    "      lr_images = lr_images[0].to(device).float()\n",
    "      \n",
    "      sr_images = model(lr_images)\n",
    "      g_loss_content = content_loss(sr_images, hr_images)\n",
    "      g_loss_content.backward()\n",
    "      \n",
    "      optimizer_gen.step()\n",
    "      \n",
    "      model.zero_grad()\n",
    "      optimizer_gen.zero_grad()\n",
    "      \n",
    "      if (i + 1) % save_interval == 1:\n",
    "        print(g_loss_content.item())\n",
    "        error_track.append(g_loss_content.item())\n",
    "        torch.save(model.state_dict(), f\"{model.name}/snapshots/epoch{epoch+1}_batch{i+1}.pt\")\n",
    "        print(f\"Saved {model.name} model and images at epoch {epoch+1}, batch {i+1} / {len(hr_data_loader)}.\")\n",
    "        save_image(sr_images, f\"{model.name}/training_images/image_epoch{epoch + 1}_batch{i+1}_sr.png\", normalize = True)\n",
    "        save_image(hr_images, f\"{model.name}/training_images/image_epoch{epoch + 1}_batch{i+1}_hr.png\", normalize = True)\n",
    "        save_image(lr_images, f\"{model.name}/training_images/image_epoch{epoch + 1}_batch{i+1}_lr.png\", normalize = True)\n",
    "  error_track_array = np.asarray(error_track)\n",
    "  error_lines.append(error_track_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
